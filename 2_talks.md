---
layout: page
title: Talks
permalink: /talks
order: 9
---


For the CMCL 2025 edition, we are glad to have invited speakers, each bringing complementary backgrounds and expertise.

## **Tessa Verhoef** (Leiden Institute of Advanced Computer Science) 
Title: *The emergence of language universals in neural agents and vision-and-language models*
### Abstract
Human cognition constrains how we communicate. Our cognitive biases and preferences interact with the processes that drive language emergence and change in non-trivial ways. A powerful method to discern the roles of cognitive biases and processes like language learning and use in shaping linguistic structure is to build agent-based models. Recent advances in computational linguistics and deep learning sparked a renewed interest in such simulations, creating the opportunity to model increasingly realistic phenomena. These models simulate emergent communication, referring to the spontaneous development of a communication system through repeated interactions between individual neural network agents. However, a crucial challenge in this line of work is that such artificial learners still often behave differently from human learners. Directly inspired by human artificial language learning studies, we proposed a novel framework for simulating language learning and change, which allows agents to first learn an artificial language and then use it to communicate, with the aim of studying the emergence of specific linguistic properties. I will show how we used it to simulate the emergence of a well-known language phenomenon: the word-order/case-marking trade-off. I will also share some recent findings where we test for the presence of a well-known human cross-modal mapping preference (the bouba-kiki effect) in vision-and-language models. Cross-modal associations play an essential role in human language understanding, learning, and evolution, but our findings reveal that multimodal language models do not align well with such human preferences. Finally, I will provide a sneak peek at new findings that reveal what a novel artificial language looks like when it has emerged to adapt to preferences of both humans and LLMs in a hybrid language game experiment. 

### Bio
Tessa Verhoef is an Assistant Professor at the Leiden Institute of Advanced Computer Science (LIACS) where she leads the Emergent Communication Group, co-founded the Creative Intelligence Lab and conducts interdisciplinary research at the intersection of language, cognition, cultural evolution and computation. She is deeply interested in the natural mechanisms that support the emergence of novel communication systems among groups of humans, computational agents, or between human and machine. Before joining Leiden University, she was a postdoc at the University of California, San Diego (UCSD), where she conducted her NWO Rubicon research at the Center for Research in Language (CRL) and became a Frontiers of Innovation Scholars Program (FISP) fellow at the departments of Communication and Electrical and Computer Engineering. She has a BSc and MSc in Artificial Intelligence, and obtained her PhD in Language Evolution at the University of Amsterdam.

## **John T. Hale** (Johns Hopkins University)
Title: *Towards mechanistic explanation of human language processing*
### Abstract
Cognitive models of human language hold out the hope of explaining how comprehension works. We hope for a process model that is intelligible to scientists and relates, in some systematic way, to the human brain. What sorts of mechanisms offer such explanation? The talk reviews candidate mechanisms of transition-based incremental parsing. Such mechanisms explicitly decide, for instance, when a word serves as a direct object of a verb. The talk makes the case for building-in this sort of interpretabilty from the start. Doing so, we gain scientific leverage on classic questions about the human sentence processing mechanism: (1) does the parser hedge its bets and (2) what is a “wrong choice"?

### Bio
John Hale started CMCL in 2010. He is currently visiting Johns Hopkins University while on leave from the University of Georgia. He also serves as a Research Scientist with Google DeepMind. He previously taught at Cornell University
and Michigan State University. He is the author of [“Automaton Theories of Human Sentence Comprehension”](https://web.stanford.edu/group/cslipublications/cslipublications/site/9781575867472.shtml).



