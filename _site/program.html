<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Program | CMCL 2024</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Program" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The 2024 Cognitive Modeling and Computational Linguistics workshop." />
<meta property="og:description" content="The 2024 Cognitive Modeling and Computational Linguistics workshop." />
<link rel="canonical" href="http://localhost:4000/program" />
<meta property="og:url" content="http://localhost:4000/program" />
<meta property="og:site_name" content="CMCL 2024" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Program" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"The 2024 Cognitive Modeling and Computational Linguistics workshop.","headline":"Program","url":"http://localhost:4000/program"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="CMCL 2024" /></head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">CMCL 2024</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/CfP">Call for Papers</a><a class="page-link" href="/program">Program</a><a class="page-link" href="/talks">Talks</a><a class="page-link" href="/organizers">Organizers</a><a class="page-link" href="/registration">Registration</a><a class="page-link" href="/code-of-conduct">Code of Conduct</a><a class="page-link" href="/previous_cmcls">Past CMCLs</a></div>
      </nav></div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Program</h1>
  </header>

  <div class="post-content">
    <h1 id="cognitive-modeling-and-computational-linguistics-cmcl-2024">Cognitive Modeling and Computational Linguistics (CMCL) 2024</h1>

<p>CMCL 2024 will have both oral presentations and poster presentations. The complete program is under definition.</p>

<h2 id="programme">Programme</h2>

<p><strong>Thursday, August 15th, 2024, <a href="https://www.worldtimebuddy.com/?pl=1&amp;lid=1609350&amp;h=1609350&amp;hf=0">Bangkok (UTC+07.00)</a></strong></p>

<p><strong>08:45 - 09:00 Opening Remarks</strong></p>

<p><strong>09:00 - 09:40 Invited Speaker 1: Sandro Pezzelle</strong> (chair: Tatsuki Kuribayashi)</p>

<p><strong>9:40 - 10:40 Session 1 (Oral Presentations)</strong> (chair: Giulia Rambelli)</p>
<ul>
  <li>11 <em>Hierarchical syntactic structure in human-like language models</em>. Michael Wolfman, Donald Dunagan, Jonathan Brennan, John T. Hale (archival)</li>
  <li>8 <em>Do large language models resemble humans in language use?</em>. Zhenguang G. Cai, Xufeng Duan, David A. Haslett, Shuqi Wang, Martin J. Pickering (archival)</li>
  <li>4 <em>Evaluating Vision-Language Models on Bistable Images</em>. Artemis Panagopoulou, Coby Melkin, Chris Callison-Burch (archival)</li>
</ul>

<p><strong>10:40 - 11:00 Coffee Break</strong></p>

<p><strong>11:00 - 12:20 Session 2 (Poster Session)</strong></p>
<ul>
  <li>3 <em>BAMBINO-LM: (Bilingual-)Human-Inspired Continual Pre-training of BabyLM</em>. Zhewen Shen, Aditya Joshi, Ruey-Cheng Chen (archival)</li>
  <li>9 <em>The Curious Case of Representational Alignment: Unravelling Visio-Linguistic Tasks in Emergent Communication</em>. Tom Kouwenhoven, Max Peeperkorn, Bram Van Dijk, Tessa Verhoef (archival)</li>
  <li>10 <em>Modeling Overregularization in Children with Small Language Models</em>. Akari Haga, Saku Sugawara, Akiyo Fukatsu, Miyu Oba, Hiroki Ouchi, Taro Watanabe, Yohei Oseki (Findings of ACL 2024)</li>
  <li>14 <em>Language models’ probability distributions are calibrated to cognitive profiles: An investigation of the predictive power of surprisal and entropy</em>. Patrick Haller, Lena Sophia Bolliger, Lena Ann Jäger (Findings of ACL 2024)</li>
  <li>16 <em>What Makes Language Models Good-enough?</em>. Daiki Asami, Saku Sugawara (Findings of ACL 2024)</li>
  <li>17 <em>Do LLMs Agree with Humans on Emotional Associations to Nonsense Words?</em>. Yui Miyakawa, Chihaya Matsuhira, Hirotaka KATO, Takatsugu Hirayama, Takahiro Komamizu, Ichiro Ide (archival)</li>
  <li>20 <em>Predict but Also Integrate: an Analysis of Sentence Processing Models for English and Hindi</em> Nina Delcaro, Luca Onnis, Raquel G. Alhama (archival)</li>
  <li>21 <em>Transformer Attention vs Human Attention in Anaphora Resolution</em>. Anastasia Kozlova, Albina Akhmetgareeva, Aigul Khanova, Semen Kudriavtsev, Alena Fenogenova (archival)</li>
  <li>22 <em>Tree-Planted Transformers: Unidirectional Transformer Language Models with Implicit Syntactic Supervision</em>. Ryo Yoshida, Taiga Someya, Yohei Oseki (Findings of ACL 2024)</li>
  <li>26 <em>How Much Does Non-verbal Communication Conform to Entropy Rate Constancy?: A Case Study on Listener Gaze in Interaction</em>. Yu Wang, Yang Xu, Gabriel Skantze, Hendrik Buschmeier (Findings of ACL 2024)</li>
  <li>27 <em>Daily auditory environments in French-speaking infants: A longitudinal dataset</em>. Estelle Hervé, Clément François, Laurent Prevot (archival)</li>
  <li>28 <em>VerbCLIP: Improving Verb Understanding in Vision-Language Models with Compositional Structures</em>. Hadi Wazni, Kin Ian Lo, Mehrnoosh Sadrzadeh (non-archival)</li>
  <li>40 <em>What does Kiki look like? Cross-modal associations between speech sounds and visual shapes in vision-and-language models</em>. Tessa Verhoef, Kiana Shahrasbi, Tom Kouwenhoven (archival)</li>
  <li>46 <em>How Useful is Context, Actually? Comparing LLMs and Humans on Discourse Marker Prediction</em>. Emily Sadlier-Brown, Millie Lou, Miikka Silfverberg, Carla L. Hudson Kam (archival)</li>
</ul>

<p><strong>12:20 - 14:00 Lunch</strong></p>

<p><strong>14:00 - 15:00 Session 3 (Oral Presentations)</strong> (chair: Giulia Rambelli)</p>
<ul>
  <li>18 <em>Large language models fail to derive atypicality inferences in a human-like manner</em>. Charlotte Kurch, Margarita Ryzhova, Vera Demberg (archival)</li>
  <li>45 <em>Diachronic change in verb usage statistics predicts differences in sentence processing across the lifespan</em>. Ellis Cain, Rachel Ryskin (archival)</li>
  <li>32 <em>How can large language models become more human?</em>. Daphne Wang, Mehrnoosh Sadrzadeh, Miloš Stanojević, Wing-Yee Chow, Richard Breheny (archival)</li>
</ul>

<p><strong>15:00 - 15:40 Invited Speaker 2: Frank Keller</strong> (chair: Yohei Oseki)</p>

<p><strong>15:40 - 16:00 Coffee Break</strong></p>

<p><strong>16:00 - 17:20	Session 4 (Poster Session)</strong></p>
<ul>
  <li>7 <em>Locally Biased Transformers Better Align with Human Reading Times</em>. Andrea Gregor de Varda, Marco Marelli (archival)</li>
  <li>19 <em>Structural Similarities Between Language Models and Neural Response Measurements</em>. Antonia Karamolegkou, Jiaang Li, Yova Kementchedjhieva, Mostafa Abdou, Sune Lehmann, Anders Søgaard  (Neurips 2023 NeurReps Workshop)</li>
  <li>24 <em>Exploring Spatial Schema Intuitions in Large Language and Vision Models</em>. Philipp Wicke, Lennart Wachowiak (Findings of ACL 2024)</li>
  <li>25 <em>Evaluating Lexical Aspect with Large Language Models</em>. Bolei Ma (archival)</li>
  <li>29 <em>Analysing and Validating Language Complexity Metrics Across South American Indigenous Languages</em>. Felipe Ribas Serras, Miguel de Mello Carpi, Matheus Castello Branco, Marcelo Finger (archival)</li>
  <li>30 <em>Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment</em>. William Merrill, Zhaofeng Wu, Norihito Naka, Yoon Kim, Tal Linzen (Findings of ACL 2024)</li>
  <li>33 <em>So many design choices: Improving and interpreting neural agent communication in signaling games</em>. Timothée Bernard, Timothee Mickus (Findings of ACL 2023)</li>
  <li>34 <em>The Emergence of High-Level Semantics in a Signaling Game</em>. Timothée Bernard, Timothee Mickus, Hiroya Takamura  (in SEM 2024)</li>
  <li>35 <em>Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test</em>. Dang Thi Thao Anh, Limor Raviv, Lukas Galke (archival)</li>
  <li>37 <em>Evaluating Grammatical Well-Formedness in Large Language Models: A Comparative Study with Human Judgments</em>. Zhuang Qiu, Xufeng Duan, Zhenguang Cai (archival)</li>
  <li>42 <em>Evaluating Semantic Relations in Predicting Textual Labels for Images of Abstract and Concrete Concepts</em>. Tarun Tater, Sabine Schulte im Walde, Diego Frassinelli (archival)</li>
  <li>49 <em>LLMs’ morphological analyses of complex FST-generated Finnish words</em>. Anssi Moisio, Mathias Creutz, Mikko Kurimo (archival)</li>
  <li>53 <em>PUB: A Pragmatics Understanding Benchmark for Assessing LLMs’ Pragmatics Capabilities</em>. Settaluri Lakshmi Sravanthi, Meet Doshi, Pavan Kalyan Tankala, Rudra Murthy, Raj Dabre, Pushpak Bhattacharyya (Findings of ACL 2024)</li>
  <li>ARR4 <em>An Eye Opener Regarding Task-Based Text Gradient Saliency</em>. Guojun Wu, Lena Sophia Bolliger, David Robert Reich, Lena Ann Jäger (archival)</li>
  <li>ARR6 <em>Improving Language Models for Emotion Analysis: Insights from Cognitive Science</em>. Constant Bonard, Gustave Cortal (archival)</li>
</ul>

<p><strong>17:20 - 18:00	Invited Speaker 3: Aida Nematzadeh</strong> (chair: Yohei Oseki)</p>

<p><strong>18:00 - 18:10	Closing Remarks</strong></p>

<!---

CMCL 2022 will have both oral presentations and poster presentations. The complete program is under definition.


## Location and Date

Dublin, Ireland and on Zoom!


<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d152515.25333408735!2d-6.385787383888776!3d53.32444313848332!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x48670e80ea27ac2f%3A0xa00c7a9973171a0!2sDublin%2C%20Ireland!5e0!3m2!1sen!2sus!4v1638508842460!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>

## Programme

**Tuesday, May 26th, 2022, Standard Irish Time**


**09:30 - 09:45 Opening Remarks**

**09:45 - 10:45 Keynote Talk by Andrea E. Martin**: "Explananda in cognitive models of language processing"

**10:45 - 11:00 Coffee Break**

**11:00 - 12:30 Session 1 (Oral Presentations)**

- *Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences.* Joshua Bensemann, Alex Yuxuan Peng, Diana Benavides Prado, Yang Chen, Neset Tan, Paul Michael Corballis, Patricia Riddle and Michael Witbrock
- *Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge.* Danny Merkx, Stefan Frank and Mirjam Ernestus
- *Visually Grounded Interpretation of Noun-Noun Compounds in English.* Inga Lang, Lonneke Van Der Plas, Malvina Nissim and Albert Gatt

**12:30 - 13:30 Lunch Break**

**13:30 - 15:00 Session 2 (Oral Presentations)**

- *A Neural Model for Compositional Word Embeddings and Sentence Processing.* Shalom Lappin and Jean-Philippe Bernardy
- *Codenames as a Game of Co-occurrence Counting.* Reka Cserhati, Istvan Kollath, Andras Kicsi and Gabor Berend
- *About Time: Do Transformers Learn Temporal Verbal Aspect?* Eleni Metheniti, Tim Van De Cruys and Nabil Hathout

**15:00 - 15:15 Coffee Break**

**15:15 - 15:30 Shared Task Presentation**

CMCL 2022 Shared Task on Multilingual and Crosslingual Prediction of Human

*Reading Behavior.* Nora Hollenstein, Emmanuele Chersoni, Cassandra L Jacobs, Yohei Oseki, Laurent Prevot and Enrico Santus


**15:30 - 17:00 Poster Session**

- *Estimating word co-occurrence probabilities from pretrained static embeddings using a log-bilinear model.* Richard Futrell
- *Predicting scalar diversity with context-driven uncertainty over alternatives.* Jennifer Hu, Roger P. Levy and Sebastian Schuster
- *Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via CLIP.* Ece Takmaz, Sandro Pezzelle and Raquel Fernandez
- *Modeling the Relationship between Input Distributions and Learning Trajectories with the Tolerance Principle.* Jordan Kodner
- *NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space.* Joseph Marvin Imperial
- *Team DMG at CMCL 2022 Shared Task: Transformer Adapters for the Multi and Cross-Lingual Prediction of Human Reading Behavior.* Ece Takmaz
- *Team UFAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models.* Sunit Bhattacharya, Rishu Kumar and Ondrej Bojar
- *HkAmsters at CMCL 2022 Shared Task: Predicting Eye-Tracking Data from a Gradient Boosting Framework with Linguistic Features.* Lavinia Salicchi, Rong Xiang and Yu-Yin Hsu
- *Poirot at CMCL 2022 Shared Task: Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models.* Harshvardhan Srivastava
- *A Bayesian approach to phases for frequency-tagged encephalography in the cognitive neuroscience of language.* Sydney Dimmock, Cian O'Donnell, Conor Houghton (extended abstract presentation)
- *Learning Non-Local Phonological Alternations via Automatic Creation of Tiers.* Caleb Belth. (extended abstract presentation)

**17:00 - 18:00 Keynote Talk by Vera Demberg**: "Recent findings in pragmatic processing and their implications for computational modelling"

**18:00 - 18:15 Closing Remarks**

--->

  </div>

</article>

      </div>
    </main>

    <footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">CMCL 2024</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">CMCL 2024</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>The 2024 Cognitive Modeling and Computational Linguistics workshop.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
