---
layout: page
title: Program
permalink: /program
order: 6
---

# Cognitive Modeling and Computational Linguistics (CMCL) 2024

To be determined

<!---

CMCL 2022 will have both oral presentations and poster presentations. The complete program is under definition.


## Location and Date

Dublin, Ireland and on Zoom!


<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d152515.25333408735!2d-6.385787383888776!3d53.32444313848332!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x48670e80ea27ac2f%3A0xa00c7a9973171a0!2sDublin%2C%20Ireland!5e0!3m2!1sen!2sus!4v1638508842460!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy"></iframe>

## Programme

**Tuesday, May 26th, 2022, Standard Irish Time**


**09:30 - 09:45 Opening Remarks**

**09:45 - 10:45 Keynote Talk by Andrea E. Martin**: "Explananda in cognitive models of language processing"

**10:45 - 11:00 Coffee Break**

**11:00 - 12:30 Session 1 (Oral Presentations)**

- *Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences.* Joshua Bensemann, Alex Yuxuan Peng, Diana Benavides Prado, Yang Chen, Neset Tan, Paul Michael Corballis, Patricia Riddle and Michael Witbrock
- *Seeing the advantage: visually grounding word embeddings to better capture human semantic knowledge.* Danny Merkx, Stefan Frank and Mirjam Ernestus
- *Visually Grounded Interpretation of Noun-Noun Compounds in English.* Inga Lang, Lonneke Van Der Plas, Malvina Nissim and Albert Gatt

**12:30 - 13:30 Lunch Break**

**13:30 - 15:00 Session 2 (Oral Presentations)**

- *A Neural Model for Compositional Word Embeddings and Sentence Processing.* Shalom Lappin and Jean-Philippe Bernardy
- *Codenames as a Game of Co-occurrence Counting.* Reka Cserhati, Istvan Kollath, Andras Kicsi and Gabor Berend
- *About Time: Do Transformers Learn Temporal Verbal Aspect?* Eleni Metheniti, Tim Van De Cruys and Nabil Hathout

**15:00 - 15:15 Coffee Break**

**15:15 - 15:30 Shared Task Presentation**

CMCL 2022 Shared Task on Multilingual and Crosslingual Prediction of Human

*Reading Behavior.* Nora Hollenstein, Emmanuele Chersoni, Cassandra L Jacobs, Yohei Oseki, Laurent Prevot and Enrico Santus


**15:30 - 17:00 Poster Session**

- *Estimating word co-occurrence probabilities from pretrained static embeddings using a log-bilinear model.* Richard Futrell
- *Predicting scalar diversity with context-driven uncertainty over alternatives.* Jennifer Hu, Roger P. Levy and Sebastian Schuster
- *Less Descriptive yet Discriminative: Quantifying the Properties of Multimodal Referring Utterances via CLIP.* Ece Takmaz, Sandro Pezzelle and Raquel Fernandez
- *Modeling the Relationship between Input Distributions and Learning Trajectories with the Tolerance Principle.* Jordan Kodner
- *NU HLT at CMCL 2022 Shared Task: Multilingual and Crosslingual Prediction of Human Reading Behavior in Universal Language Space.* Joseph Marvin Imperial
- *Team DMG at CMCL 2022 Shared Task: Transformer Adapters for the Multi and Cross-Lingual Prediction of Human Reading Behavior.* Ece Takmaz
- *Team UFAL at CMCL 2022 Shared Task: Figuring out the correct recipe for predicting Eye-Tracking features using Pretrained Language Models.* Sunit Bhattacharya, Rishu Kumar and Ondrej Bojar
- *HkAmsters at CMCL 2022 Shared Task: Predicting Eye-Tracking Data from a Gradient Boosting Framework with Linguistic Features.* Lavinia Salicchi, Rong Xiang and Yu-Yin Hsu
- *Poirot at CMCL 2022 Shared Task: Zero Shot Crosslingual Eye-Tracking Data Prediction using Multilingual Transformer Models.* Harshvardhan Srivastava
- *A Bayesian approach to phases for frequency-tagged encephalography in the cognitive neuroscience of language.* Sydney Dimmock, Cian O'Donnell, Conor Houghton (extended abstract presentation)
- *Learning Non-Local Phonological Alternations via Automatic Creation of Tiers.* Caleb Belth. (extended abstract presentation)

**17:00 - 18:00 Keynote Talk by Vera Demberg**: "Recent findings in pragmatic processing and their implications for computational modelling"

**18:00 - 18:15 Closing Remarks**

--->
